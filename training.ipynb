{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# ライブラリのインストール\n",
    "# ====================\n",
    "\n",
    "! pip install pytorch-lightning==1.8.0\n",
    "! pip install transformers==4.24.0\n",
    "! pip install sentencepiece fugashi ipadic\n",
    "! pip install datasets\n",
    "! pip install pytorch-lightning\n",
    "! pip install pyknp\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install jumanpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# ライブラリの読み込み\n",
    "# ====================\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "#rinna,早稲田\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"shunk031/livedoor-news-corpus\",\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 前処理\n",
    "# ====================\n",
    "\n",
    "# RoBERTa\n",
    "#rinna\n",
    "#model_name = \"rinna/japanese-roberta-base\"\n",
    "#tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
    "#早稲田\n",
    "#model_name = \"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\n",
    "#京大 base\n",
    "#model_name = 'ku-nlp/roberta-base-japanese-char-wwm'\n",
    "#京大 large\n",
    "#model_name = 'ku-nlp/roberta-large-japanese-char-wwm'\n",
    "#Megagon Labs RoBERTa\n",
    "#model_name = \"megagonlabs/roberta-long-japanese\"\n",
    "#ACCMS RoBERTa\n",
    "model_name = 'ku-accms/roberta-base-japanese-ssuw'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# DeBERTa\n",
    "#京大v2base\n",
    "#model_name = 'ku-nlp/deberta-v2-base-japanese'\n",
    "#京大v2large (使えない)\n",
    "#model_name = 'ku-nlp/deberta-v2-large-japanese'\n",
    "#京大v2base バッチサイズ16で行ける\n",
    "#model_name = 'ku-nlp/deberta-v3-base-japanese'\n",
    "#東大v2 base\n",
    "model_name = \"izumi-lab/deberta-v2-base-japanese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# LUKE\n",
    "model_name = \"studio-ousia/luke-japanese-base-lite\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# BigBird\n",
    "model_name = \"nlp-waseda/bigbird-base-japanese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 最大文長の設定\n",
    "max_length = 128\n",
    "\n",
    "def make_dataset(tokenizer, max_length, texts, labels):\n",
    "    dataset_for_loader = list()\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        # テキストをトークンに分割する。ただし、最大文長は \"max_length\" で指定したトークン数である。\n",
    "        # 最大文長より短い文については、 \"[PAD]\" などの特殊トークンで残りの長さを埋める。\n",
    "        # 最大文長を超える文については、はみ出す部分を無視する。\n",
    "        encoding = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        # tokenizerメソッドは辞書を返す。その辞書にラベルのIDも持たせる。\n",
    "        encoding[\"labels\"] = label\n",
    "\n",
    "        # テンソルに変換\n",
    "        encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
    "\n",
    "        # 前処理済みのデータを保存して次の文へ\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "dataset_train = make_dataset(tokenizer, max_length, [dataset[\"train\"][i][\"title\"] for i in range(len(dataset[\"train\"]))], [dataset[\"train\"][i][\"category\"] for i in range(len(dataset[\"train\"]))])\n",
    "dataset_val = make_dataset(tokenizer, max_length, [dataset[\"validation\"][i][\"title\"] for i in range(len(dataset[\"validation\"]))], [dataset[\"validation\"][i][\"category\"] for i in range(len(dataset[\"validation\"]))])\n",
    "dataset_test = make_dataset(tokenizer, max_length, [dataset[\"test\"][i][\"title\"] for i in range(len(dataset[\"test\"]))], [dataset[\"test\"][i][\"category\"] for i in range(len(dataset[\"test\"]))])\n",
    "\n",
    "# データローダを作成。訓練用データはシャッフルしながら使う。\n",
    "# 検証用と評価用は損失の勾配を計算する必要がないため、バッチサイズを大きめにとれる。\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# RoBERTaによるテキスト分類\n",
    "# ====================\n",
    "\n",
    "class RoBertaClassification(pl.LightningModule):\n",
    "\n",
    "    # モデルの読み込みなど。損失関数は自動的に設定される。\n",
    "    # num_labels == 1 -> 回帰タスクなので MSELoss()\n",
    "    # num_labels > 1 -> 分類タスクなので CrossEntropyLoss()\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()    # num_labelsとlrを保存する。例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        self.bert_sc = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # 訓練用データのバッチを受け取って損失を計算\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証用データのバッチを受け取って損失を計算\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # 評価用データのバッチを受け取って分類の正解率を計算\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # ラベルの推定\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        # 正解率の計算\n",
    "        labels = batch.pop(\"labels\")\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # 最適化手法を設定\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# DeBERTaによるテキスト分類\n",
    "# ====================\n",
    "\n",
    "class DebertaClassifier(pl.LightningModule):\n",
    "\n",
    "    # モデルの読み込みなど。損失関数は自動的に設定される。\n",
    "    # num_labels == 1 -> 回帰タスクなので MSELoss()\n",
    "    # num_labels > 1 -> 分類タスクなので CrossEntropyLoss()\n",
    "    def __init__(self, model):\n",
    "        super(DebertaClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "\n",
    "    # 訓練用データのバッチを受け取って損失を計算\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証用データのバッチを受け取って損失を計算\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # 評価用データのバッチを受け取って分類の正解率を計算\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # ラベルの推定\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        # 正解率の計算\n",
    "        labels = batch.pop(\"labels\")\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # 最適化手法を設定\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# LUKEによるテキスト分類\n",
    "# ====================\n",
    "\n",
    "class LUKEClassifier(pl.LightningModule):\n",
    "\n",
    "    # モデルの読み込みなど。損失関数は自動的に設定される。\n",
    "    # num_labels == 1 -> 回帰タスクなので MSELoss()\n",
    "    # num_labels > 1 -> 分類タスクなので CrossEntropyLoss()\n",
    "    def __init__(self, model):\n",
    "        super(LUKEClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "\n",
    "    # 訓練用データのバッチを受け取って損失を計算\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証用データのバッチを受け取って損失を計算\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # 評価用データのバッチを受け取って分類の正解率を計算\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # ラベルの推定\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        # 正解率の計算\n",
    "        labels = batch.pop(\"labels\")\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # 最適化手法を設定\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# BigBirdによるテキスト分類\n",
    "# ====================\n",
    "\n",
    "class LUKEClassifier(pl.LightningModule):\n",
    "\n",
    "    # モデルの読み込みなど。損失関数は自動的に設定される。\n",
    "    # num_labels == 1 -> 回帰タスクなので MSELoss()\n",
    "    # num_labels > 1 -> 分類タスクなので CrossEntropyLoss()\n",
    "    def __init__(self, model):\n",
    "        super(LUKEClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "\n",
    "    # 訓練用データのバッチを受け取って損失を計算\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証用データのバッチを受け取って損失を計算\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # 評価用データのバッチを受け取って分類の正解率を計算\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # ラベルの推定\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        # 正解率の計算\n",
    "        labels = batch.pop(\"labels\")\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # 最適化手法を設定\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 訓練 RoBERTa\n",
    "# ====================\n",
    "\n",
    "model = RoBertaClassification(model_name, num_labels=9, lr=1e-5)\n",
    "\n",
    "# 訓練中にモデルを保存するための設定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
    "    save_weights_only=True, dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "# 訓練\n",
    "trainer = pl.Trainer(devices=1, max_epochs=3, callbacks=[checkpoint])\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# ベストモデルの確認\n",
    "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 訓練 DeBERTa\n",
    "# ====================\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "deberta_classifier = DebertaClassifier(model)\n",
    "\n",
    "# 訓練中にモデルを保存するための設定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
    "    save_weights_only=True, dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "# 訓練\n",
    "trainer = pl.Trainer(devices=1, max_epochs=3)\n",
    "trainer.fit(deberta_classifier, dataloader_train, dataloader_val)\n",
    "# ベストモデルの確認\n",
    "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 訓練 LUKE\n",
    "# ====================\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "deberta_classifier = LUKEClassifier(model)\n",
    "\n",
    "# 訓練中にモデルを保存するための設定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
    "    save_weights_only=True, dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "# 訓練\n",
    "trainer = pl.Trainer(devices=1, max_epochs=3)\n",
    "trainer.fit(deberta_classifier, dataloader_train, dataloader_val)\n",
    "# ベストモデルの確認\n",
    "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 訓練 BigBird\n",
    "# ====================\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "deberta_classifier = LUKEClassifier(model)\n",
    "\n",
    "# 訓練中にモデルを保存するための設定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
    "    save_weights_only=True, dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "# 訓練\n",
    "trainer = pl.Trainer(devices=1, max_epochs=3)\n",
    "trainer.fit(deberta_classifier, dataloader_train, dataloader_val)\n",
    "# ベストモデルの確認\n",
    "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 評価\n",
    "# ====================\n",
    "\n",
    "trainer.test(dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 確認\n",
    "# ====================\n",
    "\n",
    "text = [dataset[\"test\"][i][\"title\"] for i in range(len(dataset[\"test\"]))]\n",
    "gold = [dataset[\"test\"][i][\"category\"] for i in range(len(dataset[\"test\"]))]\n",
    "label = ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = list()\n",
    "    for batch in dataloader_test:\n",
    "        output = model.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        preds.append(labels_predicted)\n",
    "    preds = torch.cat(preds)\n",
    "    pred_label = [label.item() for label in preds]\n",
    "\n",
    "for i in range(10):\n",
    "    print(text[i])\n",
    "    print(\"pred: %s\\tgold: %s\\n\" % (label[pred_label[i]], label[gold[i]]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
